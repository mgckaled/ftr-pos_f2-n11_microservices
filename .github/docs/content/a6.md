<!-- markdownlint-disable -->

# Aula 6: Tracing Distribuído - Observabilidade em Sistemas de Microsserviços

## Resumo Executivo

Observabilidade em arquiteturas de microsserviços constitui desafio crítico substancialmente distinto de monolitos, onde requisições seguem fluxo linear previsível através de ponto único de entrada até banco de dados e retorno. Em sistemas distribuídos, requisições atravessam múltiplos serviços através de comunicação síncrona via HTTP/gRPC e assíncrona via message brokers como Apache Kafka e RabbitMQ, criando grafos complexos de dependências onde visibilidade end-to-end torna-se essencial para diagnóstico efetivo de falhas e otimização de performance. Tracing distribuído emerge como padrão fundamental que instrumenta aplicações para capturar, correlacionar e visualizar jornada completa de requisições através de topologia distribuída, propagando contexto de rastreamento através de identificadores únicos que acompanham operação desde origem até conclusão, independentemente de quantos serviços intermediários participem do processamento. OpenTelemetry estabelece-se como framework unificado vendor-neutral para observabilidade, fornecendo APIs e SDKs padronizados que abstraem complexidade de instrumentação e integração com backends de tracing como Jaeger para visualização local durante desenvolvimento e Grafana Tempo para ambientes de produção escaláveis.

Aula demonstra implementação prática de tracing distribuído em stack Node.js utilizando auto-instrumentação do OpenTelemetry que automaticamente intercepta e rastreia operações em bibliotecas comuns incluindo HTTP, gRPC, Kafka, PostgreSQL e MongoDB sem modificações no código de aplicação, simplificando adoção através de arquivo de configuração carregado via flag `-r` do Node.js antes da execução do serviço principal. Arquitetura de tracing baseia-se em conceitos fundamentais de Traces representando jornada completa end-to-end de requisição distribuída, Spans como unidades atômicas de trabalho com timestamps de início e término que capturam operações individuais como chamadas HTTP ou queries de banco de dados, Trace ID único propagado através de todos serviços participantes garantindo correlação de spans pertencentes à mesma transação de negócio, e Span ID identificando cada operação dentro do trace com relacionamento parent-child formando árvore hierárquica de execução. Estratégias de sampling controlam volume de traces coletados em ambientes de alto tráfego, incluindo head-based sampling que decide capturar ou descartar trace no momento de criação baseando-se em taxa configurada, e tail-based sampling que analisa trace completo após conclusão para decisão inteligente baseada em critérios como presença de erros ou latência excedendo threshold, otimizando custos de armazenamento enquanto preserva traces relevantes para investigação.

## Introdução e Conceitos

### Desafios de Observabilidade em Microsserviços

Arquitetura monolítica tradicional oferece modelo de observabilidade relativamente simplificado onde requisições HTTP originadas de cliente front-end atravessam caminho determinístico: requisição atinge servidor web único, percorre camadas de aplicação incluindo roteamento, controllers, lógica de negócio e camada de acesso a dados, interage com banco de dados através de transações locais, e retorna resposta através do mesmo caminho reverso. Rastreamento de execução em monolitos beneficia-se de contexto compartilhado onde stack traces, logs e métricas naturalmente correlacionam-se através de thread ou request context local, permitindo que ferramentas de Application Performance Monitoring identifiquem gargalos de performance através de profiling de execução em processo único. Debugging de falhas em monolitos, embora não trivial, circunscreve-se a investigação dentro de base de código unificada com estado de aplicação acessível através de debuggers tradicionais e logs centralizados escritos por instância única de serviço.

Microsserviços fundamentalmente alteram esta dinâmica introduzindo distribuição física e lógica de processamento através de múltiplos serviços independentemente implantáveis, cada qual operando em processos separados frequentemente em hosts distintos ou containers efêmeros em clusters Kubernetes. Requisição inicialmente recebida por API Gateway não mais executa em isolamento mas desencadeia cascata de comunicações inter-serviços: chamada síncrona via gRPC de API Gateway para serviço de Autenticação validando JWT, requisição HTTP do serviço de Encurtamento de URLs para persistir dados em seu banco exclusivo, publicação assíncrona de evento `url.created` em tópico Kafka processado posteriormente por serviço de Analytics sem acoplamento temporal direto, e potencialmente chamadas adicionais para serviços auxiliares como geração de QR codes ou notificações. Cada hop adiciona latência acumulada que soma-se ao tempo total de resposta percebido por usuário, enquanto falhas em qualquer ponto da cadeia podem manifestar-se como timeouts ou erros genéricos no cliente sem indicação clara de origem do problema. Logs isolados em cada serviço, embora individualmente informativos, carecem de contexto unificador que correlacione entradas de log pertencentes à mesma transação de negócio distribuída através de múltiplos sistemas.

Comunicação assíncrona através de message brokers adiciona complexidade adicional ao desafio de observabilidade: serviço produtor publica mensagem em tópico Kafka e imediatamente prossegue processamento sem conhecimento de quando ou se consumidor processará evento, introduzindo desacoplamento temporal onde causa e efeito separam-se por intervalos arbitrários. Imagine cenário onde criação de URL encurtada no serviço Shortener resulta em publicação de evento processado por Analytics Service responsável por incrementar contadores de utilização, mas processamento falha devido a constraint de unicidade violada ou indisponibilidade temporária de ClickHouse. Sem instrumentação adequada, desenvolvedor investigando relatórios de usuários sobre estatísticas inconsistentes enfrentaria tarefa árdua de correlacionar manualmente timestamps de logs em Shortener identificando momento de criação de URL específica, buscando mensagens correspondentes em logs do Kafka broker para confirmar publicação bem-sucedida, e finalmente examinando logs de Analytics procurando por tentativas de processamento e eventuais exceções, processo manual demorado e propenso a erros especialmente sob pressão de incidentes de produção com impacto a usuários.

### Fundamentos de Tracing Distribuído

Tracing distribuído resolve desafio de observabilidade em sistemas distribuídos através de instrumentação que captura metadados estruturados sobre execução de operações através de múltiplos serviços, correlacionando-os através de identificadores únicos propagados através de chamadas de rede. Conceitos fundamentais estabelecidos por paper seminal Dapper do Google publicado em 2010 definem modelo hierárquico composto por Traces e Spans: Trace representa jornada completa end-to-end de requisição através de sistema distribuído desde ponto de entrada até conclusão, encapsulando todas operações executadas em todos serviços participantes, enquanto Span constitui unidade atômica de trabalho representando operação individual com nome descritivo, timestamps de início e término capturando duração, conjunto de atributos chave-valor fornecendo metadados contextuais como HTTP method e status code, e eventos timestamped registrando ocorrências pontuais durante execução de span como exceções lançadas ou checkpoints de progresso.

Trace ID constitui identificador único gerado no ponto de entrada de sistema, tipicamente UUID de 128 bits ou string hexadecimal de 32 caracteres, que propaga-se através de toda cadeia de chamadas servindo como chave de correlação que agrupa todos spans pertencentes à mesma transação distribuída. Quando API Gateway recebe requisição HTTP de cliente front-end para encurtar URL, middleware de tracing gera novo Trace ID anexando-o a contexto de requisição que propaga-se automaticamente através de chamadas subsequentes: requisição gRPC síncrona para Auth Service para validar token JWT carrega Trace ID em metadados gRPC conforme especificação W3C Trace Context, operação de escrita em banco PostgreSQL do Shortener Service registra Span associado ao mesmo Trace ID capturando query SQL e duração de execução, e evento Kafka publicado para Analytics inclui Trace ID em headers de mensagem permitindo que consumidor correlacione processamento assíncrono ao trace original mesmo quando separado por minutos ou horas de latência temporal.

Span ID identifica univocamente cada operação individual dentro de trace, enquanto Parent Span ID estabelece relacionamento hierárquico formando árvore de execução onde span raiz iniciado no API Gateway não possui parent, spans filhos criados para chamadas downstream referenciam span que iniciou operação, e visualização resultante apresenta estrutura de flame graph mostrando paralelização de operações concorrentes e dependencies sequenciais através de indentação e alinhamento temporal. Atributos estruturados anexados a spans fornecem dimensionalidade para análise e filtragem: atributos semânticos padronizados por OpenTelemetry Semantic Conventions incluem `http.method`, `http.status_code`, `http.url` para operações HTTP, `db.system`, `db.statement`, `db.operation` para queries de banco de dados, e `messaging.system`, `messaging.destination`, `messaging.operation` para interações com message brokers, garantindo consistência cross-platform que habilita ferramentas de análise a entender significado de spans independentemente de linguagem de programação ou framework utilizado por serviço.

### OpenTelemetry como Padrão Unificado

OpenTelemetry emerge da convergência de projetos anteriores OpenTracing e OpenCensus como framework vendor-neutral unificado para observabilidade, graduado pela Cloud Native Computing Foundation em 2021 como especificação completa abrangendo APIs, SDKs, ferramentas de instrumentação e convenções semânticas para traces, métricas e logs. Arquitetura modular de OpenTelemetry separa responsabilidades entre API fornecendo interfaces de programação estáveis que desenvolvedores utilizam para instrumentar código sem acoplamento a implementações específicas, SDK implementando APIs e fornecendo funcionalidade de processamento, sampling e exportação de telemetria, e Instrumentações que automaticamente interceptam bibliotecas populares adicionando spans sem modificações em código de aplicação através de técnicas como monkey patching em Node.js ou bytecode instrumentation em JVM.

Node.js beneficia-se de ecossistema maduro de auto-instrumentação através do pacote `@opentelemetry/auto-instrumentations-node` que agrega instrumentações para dezenas de bibliotecas amplamente utilizadas incluindo frameworks web Express e Fastify, clientes HTTP nativos `http` e `https` além de `fetch`, clientes de banco de dados para PostgreSQL via `pg`, MongoDB, MySQL e Redis, clientes de message brokers incluindo `kafkajs` para Apache Kafka e `amqplib` para RabbitMQ, e clientes gRPC tanto `@grpc/grpc-js` quanto legado `grpc`. Instrumentação automática opera através de hooks de ciclo de vida de Node.js que interceptam carregamento de módulos, envolvendo funções críticas em wrappers que criam spans automaticamente: requisição HTTP recebida por servidor Express resulta em criação de span com nome `GET /api/urls/:id`, capturando automaticamente atributos como método HTTP, URL completa, headers relevantes, status code de resposta e duração total de processamento, enquanto query SQL executada via biblioteca `pg` gera span filho com statement SQL sanitizado removendo valores literais sensíveis e duração de execução em banco de dados.

OpenTelemetry SDK para Node.js inicializa-se através de arquivo de configuração carregado antes de código de aplicação via flag `-r` do Node.js, pattern que garante que instrumentação ativa antes de qualquer módulo aplicação carregar evitando race conditions onde bibliotecas já carregadas escapariam de instrumentação. Arquivo `tracing.js` tipicamente configura `NodeSDK` especificando nome de serviço através de Resource Attributes conforme convenção semântica `ATTR_SERVICE_NAME`, registra instrumentações automáticas via `getNodeAutoInstrumentations()`, configura exportador de traces determinando destino de telemetria como `ConsoleSpanExporter` para desenvolvimento local imprimindo spans em stdout ou `OTLPTraceExporter` enviando traces via protocolo OTLP para collectors como Jaeger ou Grafana Tempo, e implementa graceful shutdown através de handler `SIGTERM` garantindo que spans pendentes em buffer exportem-se antes de processo terminar evitando perda de telemetria durante deployments ou scale-down de containers.

```typescript
// tracing.ts
import { NodeSDK } from '@opentelemetry/sdk-node';
import { getNodeAutoInstrumentations } from '@opentelemetry/auto-instrumentations-node';
import { OTLPTraceExporter } from '@opentelemetry/exporter-trace-otlp-http';
import { Resource } from '@opentelemetry/resources';
import { ATTR_SERVICE_NAME, ATTR_SERVICE_VERSION } from '@opentelemetry/semantic-conventions';
import { BatchSpanProcessor } from '@opentelemetry/sdk-trace-base';
import { W3CTraceContextPropagator } from '@opentelemetry/core';

const resource = Resource.default().merge(
  new Resource({
    [ATTR_SERVICE_NAME]: 'shortener-service',
    [ATTR_SERVICE_VERSION]: '1.2.0',
  })
);

const traceExporter = new OTLPTraceExporter({
  url: process.env.OTEL_EXPORTER_OTLP_ENDPOINT || 'http://localhost:4318/v1/traces',
  headers: {},
});

const sdk = new NodeSDK({
  resource,
  traceExporter,
  spanProcessor: new BatchSpanProcessor(traceExporter, {
    maxQueueSize: 1000,
    maxExportBatchSize: 512,
    scheduledDelayMillis: 5000,
    exportTimeoutMillis: 30000,
  }),
  instrumentations: [
    getNodeAutoInstrumentations({
      '@opentelemetry/instrumentation-fs': { enabled: false },
      '@opentelemetry/instrumentation-http': {
        ignoreIncomingPaths: ['/health', '/metrics'],
      },
      '@opentelemetry/instrumentation-kafkajs': { enabled: true },
      '@opentelemetry/instrumentation-pg': { enabled: true },
    }),
  ],
  textMapPropagator: new W3CTraceContextPropagator(),
});

sdk.start();

process.on('SIGTERM', () => {
  sdk
    .shutdown()
    .then(() => console.log('OpenTelemetry SDK shut down successfully'))
    .catch((error) => console.error('Error shutting down OpenTelemetry SDK', error))
    .finally(() => process.exit(0));
});
```

Execução de aplicação com tracing habilitado requer apenas inclusão de flag `-r` apontando para arquivo de configuração antes de ponto de entrada principal: `node -r ./tracing.js dist/server.js` ou através de script npm como `"start": "node -r ./tracing.ts --loader tsx src/server.ts"` quando utilizando TypeScript com runtime loader. Abordagem alternativa utiliza variáveis de ambiente padronizadas por especificação OpenTelemetry permitindo configuração sem modificações em código: `OTEL_SERVICE_NAME=shortener-service OTEL_EXPORTER_OTLP_ENDPOINT=http://jaeger:4318 node -r @opentelemetry/auto-instrumentations-node/register dist/server.js`, pattern particularmente útil em ambientes containerizados onde configuração injeta-se via ConfigMaps e Secrets do Kubernetes.

## Propagação de Contexto através de Protocolos

### W3C Trace Context para HTTP

Propagação de contexto de tracing através de boundaries de serviços constitui requisito fundamental para correlação de spans pertencentes ao mesmo trace distribuído, implementada através de injeção de metadados de rastreamento em headers HTTP, metadados gRPC ou headers de mensagens Kafka conforme protocolo de comunicação utilizado. W3C Trace Context estabelece especificação padronizada definindo formato de headers HTTP que carregam informação de contexto: header `traceparent` contém representação compacta de Trace ID, Span ID do span pai, e flags de sampling em formato `00-<trace-id>-<parent-span-id>-<trace-flags>` como `00-4bf92f3577b34da6a3ce929d0e0e4736-00f067aa0ba902b7-01`, enquanto header opcional `tracestate` transporta metadados vendor-specific permitindo que sistemas de tracing proprietários propaguem informações adicionais sem conflitos através de formato chave-valor como `congo=t61rcWkgMzE,rojo=00f067aa0ba902b7`.

OpenTelemetry SDK automaticamente injeta headers `traceparent` e `tracestate` em requisições HTTP outbound através de instrumentação de clientes HTTP, garantindo que serviços downstream recebam contexto necessário para criar spans filhos correlacionados. Quando Shortener Service realiza chamada HTTP para serviço externo de geração de QR codes, middleware de instrumentação HTTP extrai contexto de trace ativo na thread de execução, serializa Trace ID e Span ID conforme formato W3C, e adiciona headers antes de enviar requisição através de rede. Serviço receptor instrumentado com OpenTelemetry automaticamente extrai contexto de headers `traceparent` recebidos através de propagator configurado, restaura contexto de trace permitindo que spans criados durante processamento de requisição associem-se ao trace original, e cria novo span com parent ID referenciando span do serviço chamador estabelecendo relacionamento hierárquico visível em visualização de trace.

Configuração de propagators controla quais formatos de contexto SDK suporta durante injeção e extração, permitindo interoperabilidade com sistemas legados: `W3CTraceContextPropagator` implementa especificação W3C moderna adotada como padrão por comunidade cloud-native, `B3Propagator` suporta formato Zipkin B3 amplamente utilizado em ecossistemas Spring Boot e Istio através de headers `X-B3-TraceId`, `X-B3-SpanId` e `X-B3-Sampled`, e `CompositePropagator` combina múltiplos propagators permitindo que serviço simultaneamente injete contexto em ambos formatos W3C e B3 garantindo compatibilidade durante migrações graduais de stack de observabilidade. Configuração via `textMapPropagator` em `NodeSDK` define strategy global aplicada automaticamente por todas instrumentações: `new W3CTraceContextPropagator()` para ambientes greenfield, `new CompositePropagator({ propagators: [new W3CTraceContextPropagator(), new B3Propagator()] })` para interoperabilidade com sistemas heterogêneos.

```typescript
import { W3CTraceContextPropagator } from '@opentelemetry/core';
import { B3Propagator, B3InjectEncoding } from '@opentelemetry/propagator-b3';
import { CompositePropagator } from '@opentelemetry/core';
import { propagation } from '@opentelemetry/api';

const compositePropagator = new CompositePropagator({
  propagators: [
    new W3CTraceContextPropagator(),
    new B3Propagator({ injectEncoding: B3InjectEncoding.MULTI_HEADER }),
  ],
});

propagation.setGlobalPropagator(compositePropagator);
```

### Propagação via Kafka e Message Brokers

Comunicação assíncrona através de message brokers introduz desafio adicional para propagação de contexto devido à natureza fire-and-forget onde produtor publica mensagem e imediatamente prossegue sem conhecimento de quando consumidor processará evento, potencialmente horas ou dias após publicação. Instrumentação de KafkaJS automaticamente injeta contexto de trace em headers de mensagens Kafka permitindo que consumidores correlacionem processamento assíncrono ao trace original: produtor com contexto de trace ativo serializa Trace ID e Span ID em headers de mensagem usando keys padronizados como `traceparent` análogo a HTTP, publica mensagem em tópico Kafka com headers anexados à estrutura de mensagem, e consumidor subscrito ao tópico extrai contexto de headers recebidos restaurando Trace ID original permitindo que spans criados durante processamento de evento associem-se ao trace iniciado no serviço upstream.

Visualização de traces incluindo comunicação assíncrona apresenta spans de produção e consumo como operações distintas correlacionadas por Trace ID compartilhado: span `kafka.send` no Shortener Service captura duração de publicação de evento `url.created` incluindo serialização de payload e confirmação de ack do broker, enquanto span `kafka.receive` no Analytics Service registra timestamp de recebimento de mensagem potencialmente minutos após publicação. Span de processamento de consumer cria-se como filho do span de recebimento, capturando lógica de negócio executada como incremento de contador em ClickHouse, e relacionamento parent estabelece-se através de Parent Span ID referenciando span de envio original criando link causal entre produtor e consumidor visível em flame graph que demonstra latência assíncrona através de gap temporal entre conclusão de span de send e início de span de receive.

```typescript
import { trace, context, propagation } from '@opentelemetry/api';
import { Kafka, Producer, Consumer, EachMessagePayload } from 'kafkajs';

const kafka = new Kafka({
  clientId: 'shortener-service',
  brokers: ['localhost:9092'],
});

const producer: Producer = kafka.producer();

async function publishUrlCreatedEvent(url: { id: string; shortCode: string; originalUrl: string }) {
  const span = trace.getActiveSpan();

  const headers: Record<string, string> = {};
  propagation.inject(context.active(), headers);

  await producer.send({
    topic: 'url.created',
    messages: [
      {
        key: url.id,
        value: JSON.stringify(url),
        headers: Object.entries(headers).reduce((acc, [key, value]) => {
          acc[key] = Buffer.from(value);
          return acc;
        }, {} as Record<string, Buffer>),
      },
    ],
  });
}

const consumer: Consumer = kafka.consumer({ groupId: 'analytics-service' });

async function handleMessage({ topic, partition, message }: EachMessagePayload) {
  const headers: Record<string, string> = {};
  if (message.headers) {
    Object.entries(message.headers).forEach(([key, value]) => {
      if (value) headers[key] = value.toString();
    });
  }

  const extractedContext = propagation.extract(context.active(), headers);

  await context.with(extractedContext, async () => {
    const tracer = trace.getTracer('analytics-service');
    const span = tracer.startSpan('process_url_created_event');

    try {
      const payload = JSON.parse(message.value!.toString());
      // Processar evento incrementando contador em ClickHouse
      span.setAttributes({
        'messaging.system': 'kafka',
        'messaging.destination': topic,
        'messaging.operation': 'process',
        'url.id': payload.id,
      });
    } finally {
      span.end();
    }
  });
}
```

## Ferramentas de Visualização e Análise

### Jaeger para Desenvolvimento Local

Jaeger constitui plataforma open-source de tracing distribuído originalmente desenvolvida por Uber Technologies e doada à Cloud Native Computing Foundation, alcançando status de projeto graduado após ampla adoção em comunidade cloud-native. Arquitetura tradicional de Jaeger v1 compõe-se de múltiplos componentes: Jaeger Agent opera como daemon em cada host recebendo spans via UDP de aplicações instrumentadas e encaminhando para Collector em batches reduzindo overhead de rede, Jaeger Collector valida e transforma spans recebidos escrevendo-os em backend de storage configurado, Storage Backend persiste spans em tecnologias como Cassandra, Elasticsearch, ClickHouse ou in-memory para desenvolvimento, e Jaeger Query expõe API e UI web para busca e visualização de traces permitindo desenvolvedores navegarem hierarquia de spans e analisarem distribuição de latência.

Jaeger v2 introduz arquitetura simplificada baseada em OpenTelemetry Collector consolidando Agent, Collector e Query em binário único que recebe traces via protocolo OTLP padronizado, elimina necessidade de deployment de múltiplos componentes reduzindo complexidade operacional, e mantém compatibilidade com backends de storage existentes através de interfaces pluggáveis. Deployment local de Jaeger para desenvolvimento tipicamente utiliza imagem Docker `jaegertracing/all-in-one` que empacota todos componentes incluindo UI web em container único com storage in-memory efêmero: `docker run -d --name jaeger -p 16686:16686 -p 4318:4318 jaegertracing/all-in-one:latest` expõe UI em `http://localhost:16686` e endpoint OTLP HTTP em porta 4318 onde aplicações instrumentadas enviam traces.

Interface web de Jaeger fornece capacidades essenciais para debugging e análise de performance: página Search permite filtragem de traces por serviço, operação, tags customizados, duração mínima e range de timestamps retornando lista de traces ordenados cronologicamente, visualização de Trace Detail apresenta flame graph hierárquico mostrando spans como barras horizontais posicionadas temporalmente com largura proporcional à duração onde indentação representa relacionamento parent-child e cores distinguem diferentes serviços, e painel de Span Details revela atributos estruturados incluindo HTTP status codes, queries SQL, stack traces de exceções e eventos timestamped registrados durante execução. Análise de latência identifica gargalos através de ordenação de spans por duração revelando operações lentas: query de banco de dados consumindo 800ms de requisição total de 950ms indica candidato óbvio para otimização via adição de índices ou caching, enquanto múltiplas chamadas sequenciais a serviços externos sugerem oportunidade de paralelização reduzindo latência total.

### Grafana Tempo para Produção

Grafana Tempo emerge como backend de tracing distribuído otimizado para custos em ambientes de produção através de arquitetura que armazena traces em object storage econômico como S3, GCS ou Azure Blob Storage ao invés de bancos de dados especializados caros, reduzindo custos operacionais em ordem de magnitude enquanto mantém capacidade de query eficiente através de índices compactos em bancos key-value como BadgerDB ou Redis. Tempo integra-se nativamente com ecossistema Grafana permitindo que dashboards unifiquem visualização de métricas coletadas por Prometheus, logs agregados por Loki e traces capturados por Tempo em interface única, habilitando workflows de investigação onde desenvolvedores iniciam análise examinando métricas de latência em dashboard, perfuram para logs filtrados por timerange de spike de latência, e finalmente navegam para traces específicos clicando em Trace IDs extraídos de entradas de log estruturadas.

Arquitetura distribuída de Tempo separa responsabilidades através de componentes especializados: Distributor recebe traces via OTLP, Zipkin ou Jaeger protocols, valida formato e distribui para Ingestor usando consistent hashing garantindo que todos spans de mesmo Trace ID direcionem-se ao mesmo Ingestor, Ingestor mantém traces recentes em memória e buffer local escrevendo blocos compactados periodicamente em object storage após window temporal configurável, Compactor executa background jobs que consolidam blocos pequenos em arquivos maiores otimizando layout de storage e reduzindo custos de requisições em object storage, e Querier processa queries de busca lendo índices compactos e hidratando spans de object storage sob demanda. Deployment em Kubernetes utiliza Helm charts oficiais configurando StatefulSets para Ingestors que requerem persistência temporária de buffers e Deployments para Distributors e Queriers stateless escalados horizontalmente baseando-se em carga de ingest e query respectivamente.

Integração entre Loki e Tempo habilita navegação fluida de logs para traces através de derived fields que automaticamente extraem Trace IDs de entradas de log estruturadas e renderizam como links clicáveis: log entry `{"level":"info","msg":"Created shortened URL","traceId":"4bf92f3577b34da6a3ce929d0e0e4736"}` apresenta `traceId` como hyperlink que abre visualização de trace correspondente em painel lateral sem necessidade de copiar identificador manualmente e navegar para ferramenta separada. Configuração em Grafana data source de Loki adiciona derived field através de regex extraction ou JSON path query especificando que campo `traceId` em logs JSON deve vincular-se a data source Tempo: pattern `(?:traceId|trace_id)=(\w+)` extrai identificador de formatos comuns e gera link para `http://tempo/trace/${__value.raw}` permitindo navegação contextual que acelera drasticamente workflows de troubleshooting.

```yaml
# tempo-config.yaml
apiVersion: 1
datasources:
  - name: Tempo
    type: tempo
    access: proxy
    url: http://tempo:3100
    jsonData:
      tracesToLogs:
        datasourceUid: 'loki'
        tags: ['service.name', 'http.status_code']
        mappedTags: [{ key: 'service.name', value: 'service' }]
        filterByTraceID: true
        filterBySpanID: false
      tracesToMetrics:
        datasourceUid: 'prometheus'
        tags: [{ key: 'service.name', value: 'service' }]
        queries:
          - name: 'Request Rate'
            query: 'rate(http_requests_total{service="$__tags"}[5m])'
      serviceMap:
        datasourceUid: 'prometheus'
```

## Estratégias de Sampling

### Head-Based Sampling

Volume de traces gerado por aplicações em produção processando milhares de requisições por segundo rapidamente torna-se insustentável tanto do ponto de vista de overhead de performance quanto custos de armazenamento: capturar 100% de traces em serviço processando 10.000 req/s com média de 50 spans por trace gera 500.000 spans por segundo ou 43 bilhões de spans por dia, consumindo terabytes de storage e introduzindo latência perceptível devido a overhead de serialização e transmissão de telemetria. Sampling resolve trade-off entre visibilidade completa e custos operacionais através de decisão algorítmica sobre quais traces capturar, balanceando representatividade estatística que preserva capacidade de identificar padrões de performance e anomalias com redução de volume de dados coletados e armazenados.

Head-based sampling implementa decisão de sampling no ponto de entrada de sistema quando trace inicia-se, aplicando algoritmo determinístico ou probabilístico que decide imediatamente se trace será capturado completamente ou descartado: `ParentBasedSampler` respeita decisão de sampling propagada de serviço upstream através de trace flags em header `traceparent` garantindo consistência onde todos serviços participantes de trace campturado também exportam seus spans, `TraceIdRatioBased` aplica sampling probabilístico usando bits do Trace ID como fonte de randomness configurável através de taxa como 0.1 capturando 10% de traces selecionados uniformemente através de espaço de identificadores, e `AlwaysOnSampler` ou `AlwaysOffSampler` forçam captura ou descarte total úteis para debugging em staging ou desabilitação completa de tracing em ambientes específicos.

Configuração de head-based sampling define-se durante inicialização de SDK através de opções de `TracerProvider`: sampling rate de 10% reduz volume de spans exportados em 90% ao custo de visibilidade estatística limitada a subset representativo de tráfego, adequado para sistemas com comportamento homogêneo onde análise de amostra reflete características de população completa. Limitação fundamental de head-based sampling reside em incapacidade de aplicar lógica de decisão baseada em características de trace completo: não é possível priorizar captura de traces contendo erros ou latência anormal pois decisão ocorre antes de conhecimento de outcomes de processamento, resultando em cenário onde maioria de traces descartados pode incluir justamente exemplos de falhas raras que seriam mais valiosos para investigação, enquanto traces bem-sucedidos de baixa latência dominam amostra capturada fornecendo visibilidade limitada sobre problemas reais.

```typescript
import { NodeSDK } from '@opentelemetry/sdk-node';
import {
  ParentBasedSampler,
  TraceIdRatioBasedSampler,
  AlwaysOnSampler
} from '@opentelemetry/sdk-trace-base';

const sdk = new NodeSDK({
  sampler: new ParentBasedSampler({
    root: new TraceIdRatioBasedSampler(0.1), // 10% de traces
  }),
  // ... outras configurações
});

// Sampling adaptativo baseado em ambiente
const samplingRate = process.env.NODE_ENV === 'production' ? 0.01 : 1.0;
const sampler = process.env.NODE_ENV === 'production'
  ? new ParentBasedSampler({ root: new TraceIdRatioBasedSampler(samplingRate) })
  : new AlwaysOnSampler();
```

### Tail-Based Sampling

Tail-based sampling supera limitações de head-based sampling diferindo decisão até conclusão completa de trace, permitindo aplicação de lógica sofisticada que analisa características de execução real antes de determinar se preservar ou descartar trace. Implementação requer buffer temporário onde todos spans de trace armazenam-se em memória ou storage temporário até que span raiz conclua-se e todos spans filhos sejam recebidos, momento em que sampler processor analisa trace completo aplicando políticas configuráveis e decide exportar para backend permanente ou descartar liberando recursos de buffer. Arquitetura típica utiliza OpenTelemetry Collector deployment como layer intermediário entre aplicações e backend de tracing: aplicações instrumentadas exportam 100% de spans para Collector local ou regional, Collector acumula spans em buffer agrupando por Trace ID até timeout ou recebimento de span final, tail-based sampling processor aplica regras de decisão analisando agregados de trace, e Collector exporta subset filtrado para backend permanente como Tempo ou Jaeger reduzindo volume armazenado enquanto preserva traces relevantes.

Políticas de tail-based sampling implementam lógica de negócio que prioriza traces de maior valor para investigação: política `error` preserva automaticamente 100% de traces contendo pelo menos um span com erro identificado através de status code ou atributos de exceção, garantindo que falhas capturam-se independentemente de quão raras sejam, política `latency` mantém traces excedendo threshold configurado como 95º percentil de distribuição de latência capturando exemplos de performance anormal que indicam gargalos, política `probabilistic` aplica sampling rate base como 1% a traces não capturados por outras políticas garantindo amostra representativa de tráfego normal, e política `rate_limiting` limita quantidade absoluta de traces por segundo exportados evitando sobrecarga de backend durante spikes de tráfego enquanto priorizando traces de alta prioridade identificados por políticas anteriores.

```yaml
# otel-collector-config.yaml
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318

processors:
  tail_sampling:
    decision_wait: 10s
    num_traces: 100000
    expected_new_traces_per_sec: 1000
    policies:
      - name: errors
        type: status_code
        status_code: {status_codes: [ERROR]}
      - name: slow_requests
        type: latency
        latency: {threshold_ms: 1000}
      - name: critical_endpoints
        type: string_attribute
        string_attribute: {key: http.route, values: [/api/payment, /api/orders]}
      - name: sample_baseline
        type: probabilistic
        probabilistic: {sampling_percentage: 1}
      - name: rate_limit
        type: rate_limiting
        rate_limiting: {spans_per_second: 500}

exporters:
  otlp/tempo:
    endpoint: tempo:4317
    tls:
      insecure: true

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [tail_sampling]
      exporters: [otlp/tempo]
```

## Padrões Avançados e Boas Práticas

### Correlation IDs e Log-Trace Correlation

Correlação efetiva entre logs e traces amplifica valor de ambos sinais de observabilidade permitindo que desenvolvedores naveguem fluidamente entre contextos: análise de trace identifica span específico com erro mas carece de detalhes de estado de aplicação, link para logs filtrados por Trace ID e timestamp de span revela stack trace completo e valores de variáveis capturados em log entries correlacionadas, e inversamente investigação iniciada através de busca textual em logs identifica mensagens de erro contendo Trace ID que habilita visualização de contexto distribuído completo mostrando quais serviços participaram de requisição e onde latência acumulou-se. Implementação requer injeção de Trace ID e Span ID em structured logs através de integração entre OpenTelemetry API e biblioteca de logging utilizada por aplicação.

Winston e Pino, bibliotecas populares de logging estruturado em Node.js, suportam formatters customizados que automaticamente extraem contexto de trace ativo e incluem como campos em log entries: middleware de logging captura `trace.getActiveSpan()` obtendo referência a span atual, extrai Trace ID e Span ID via `span.spanContext()`, e injeta como campos `trace_id` e `span_id` em objeto de log que serializa-se como JSON formatado enviado para stdout capturado por log aggregators como Fluentd ou Promtail. Configuração de data source Loki em Grafana adiciona derived fields que reconhecem campos `trace_id` em logs JSON e renderizam como links clicáveis que abrem trace correspondente em painel Tempo adjacente, habilitando workflow onde desenvolvedor busca logs por mensagem de erro textual, visualiza resultados filtrados com links de trace inline, e clica para visualizar contexto distribuído completo sem trocar de ferramenta ou copiar identificadores manualmente.

```typescript
import winston from 'winston';
import { trace, context } from '@opentelemetry/api';

const logger = winston.createLogger({
  level: 'info',
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format((info) => {
      const span = trace.getSpan(context.active());
      if (span) {
        const spanContext = span.spanContext();
        info.trace_id = spanContext.traceId;
        info.span_id = spanContext.spanId;
        info.trace_flags = spanContext.traceFlags;
      }
      return info;
    })(),
    winston.format.json()
  ),
  transports: [new winston.transports.Console()],
});

// Uso em aplicação
app.post('/api/urls', async (req, res) => {
  logger.info('Creating shortened URL', {
    originalUrl: req.body.url,
    userId: req.user.id
  });

  try {
    const shortUrl = await urlService.createShortUrl(req.body.url);
    logger.info('Successfully created shortened URL', {
      shortCode: shortUrl.code
    });
    res.json(shortUrl);
  } catch (error) {
    logger.error('Failed to create shortened URL', {
      error: error.message,
      stack: error.stack
    });
    res.status(500).json({ error: 'Internal server error' });
  }
});
```

### Span Events e Annotations

Spans capuram duração de operações através de timestamps de início e término mas frequentemente ocorrem eventos pontuais durante execução que fornecem contexto valioso para debugging: exceção lançada e capturada durante retry de chamada de rede, checkpoint de progresso em processamento de batch indicando quantos registros processaram-se, ou alocação de recurso externo como lock distribuído adquirido temporariamente. Span Events fornecem mecanismo para registrar ocorrências timestamped dentro de lifecycle de span sem criar spans filhos que introduziriam overhead desnecessário: evento captura nome descritivo, timestamp preciso de ocorrência, e atributos estruturados fornecendo contexto adicional, aparecendo em visualização de trace como marcadores inline ao longo de barra de span permitindo identificar exatamente quando durante execução de operação evento ocorreu.

OpenTelemetry API expõe método `span.addEvent()` que aceita nome de evento e opcionalmente atributos e timestamp customizado, tipicamente invocado durante tratamento de exceções ou checkpoints de lógica de negócio: processamento de mensagem Kafka que executa transformação complexa registra eventos marcando progresso através de estágios de pipeline, operação de database query que sofre retry devido a deadlock registra evento capturando número de tentativa e erro específico recebido, e operação de network request intercepta exceção de timeout registrando evento antes de iniciar retry com timeout aumentado. Convenção semântica define eventos padronizados como `exception` para erros capturados incluindo atributos obrigatórios `exception.type`, `exception.message` e `exception.stacktrace` facilitando análise automatizada de traces identificando padrões de falhas através de agregação de eventos de exceção por tipo.

```typescript
import { trace, SpanStatusCode } from '@opentelemetry/api';

async function processUrlCreatedEvent(message: KafkaMessage) {
  const tracer = trace.getTracer('analytics-service');
  const span = tracer.startSpan('process_url_created');

  try {
    span.addEvent('event_received', {
      'message.offset': message.offset,
      'message.size': message.value?.length,
    });

    const payload = JSON.parse(message.value!.toString());
    span.addEvent('payload_parsed');

    // Simular validação com potencial falha
    if (!payload.id || !payload.shortCode) {
      span.addEvent('validation_failed', {
        'validation.missing_fields': JSON.stringify(
          Object.keys({ id: payload.id, shortCode: payload.shortCode })
            .filter(k => !payload[k])
        ),
      });
      throw new Error('Invalid payload');
    }

    span.addEvent('validation_passed');

    await clickhouseClient.insert({
      table: 'url_analytics',
      values: [{ url_id: payload.id, created_at: new Date() }],
    });

    span.addEvent('inserted_to_clickhouse', {
      'db.table': 'url_analytics',
      'url.id': payload.id,
    });

    span.setStatus({ code: SpanStatusCode.OK });
  } catch (error) {
    span.recordException(error);
    span.setStatus({
      code: SpanStatusCode.ERROR,
      message: error.message
    });
    span.addEvent('processing_failed', {
      'error.type': error.constructor.name,
      'error.handled': 'true',
    });
    throw error;
  } finally {
    span.end();
  }
}
```

### Instrumentação Manual para Lógica de Negócio

Auto-instrumentação fornece visibilidade automática sobre operações de infraestrutura como HTTP requests, database queries e message broker interactions mas carece de contexto sobre lógica de negócio específica de domínio: trace de criação de URL encurtada mostra spans para requisição HTTP recebida e insert em PostgreSQL mas não captura operações intermediárias como validação de unicidade de shortcode, geração de QR code ou verificação de URL contra lista de bloqueio. Instrumentação manual complementa auto-instrumentação através de criação explícita de spans customizados que representam operações de negócio significativas, fornecendo nomenclatura semântica que reflete domínio de aplicação e atributos que capturam entidades e decisões específicas de contexto.

OpenTelemetry API fornece interface `Tracer` obtida via `trace.getTracer('service-name')` que expõe método `startSpan()` criando novo span filho do span ativo automaticamente, aceita nome descritivo de operação e opções incluindo atributos iniciais e kind de span distinguindo CLIENT para chamadas outbound, SERVER para requests recebidos, INTERNAL para operações puramente locais, PRODUCER para publicação em message broker e CONSUMER para processamento de mensagens. Pattern típico envolve iniciar span no início de função ou bloco de código crítico, executar lógica de negócio capturando potenciais exceções, adicionar atributos dinâmicos baseando-se em resultados de processamento, e finalmente chamar `span.end()` em bloco `finally` garantindo que span conclua-se mesmo durante exceções evitando vazamento de memória de spans nunca finalizados.

```typescript
import { trace, SpanKind, SpanStatusCode } from '@opentelemetry/api';

class UrlShortenerService {
  private tracer = trace.getTracer('shortener-service');

  async createShortUrl(originalUrl: string, userId: string): Promise<ShortUrl> {
    const span = this.tracer.startSpan('create_short_url', {
      kind: SpanKind.INTERNAL,
      attributes: {
        'url.original': originalUrl,
        'user.id': userId,
      },
    });

    return await trace.with(trace.setSpan(trace.active(), span), async () => {
      try {
        // Validação de URL
        const validationSpan = this.tracer.startSpan('validate_url');
        const isValid = await this.validateUrl(originalUrl);
        validationSpan.setAttributes({
          'validation.result': isValid,
          'validation.provider': 'google-safe-browsing',
        });
        validationSpan.end();

        if (!isValid) {
          span.setStatus({
            code: SpanStatusCode.ERROR,
            message: 'URL blocked by safe browsing'
          });
          throw new Error('Invalid URL');
        }

        // Geração de shortcode
        const shortCode = await this.generateShortCode();
        span.setAttribute('url.short_code', shortCode);

        // Verificação de unicidade
        const uniqueCheckSpan = this.tracer.startSpan('check_shortcode_uniqueness');
        const isUnique = await this.isShortCodeUnique(shortCode);
        uniqueCheckSpan.setAttribute('shortcode.is_unique', isUnique);
        uniqueCheckSpan.end();

        if (!isUnique) {
          span.addEvent('shortcode_collision', {
            'shortcode': shortCode
          });
          return this.createShortUrl(originalUrl, userId); // Retry
        }

        // Persistência
        const shortUrl = await this.urlRepository.save({
          shortCode,
          originalUrl,
          userId,
          createdAt: new Date(),
        });

        span.setAttributes({
          'url.id': shortUrl.id,
          'url.short_code': shortUrl.shortCode,
        });

        // Publicação de evento
        await this.eventPublisher.publish('url.created', {
          id: shortUrl.id,
          shortCode: shortUrl.shortCode,
          originalUrl: shortUrl.originalUrl,
        });

        span.setStatus({ code: SpanStatusCode.OK });
        return shortUrl;
      } catch (error) {
        span.recordException(error);
        span.setStatus({
          code: SpanStatusCode.ERROR,
          message: error.message
        });
        throw error;
      } finally {
        span.end();
      }
    });
  }
}
```

## Observabilidade em Service Mesh

### Istio Envoy Sidecar Tracing

Service mesh como Istio e Linkerd introduzem camada de infraestrutura dedicada para comunicação inter-serviços através de injeção de sidecar proxies que interceptam todo tráfego de rede entrante e sainte de pods, fornecendo funcionalidades transversais incluindo mutual TLS automático, retry policies, circuit breaking e crucialmente distributed tracing sem necessidade de instrumentação explícita em código de aplicação. Istio utiliza Envoy proxy como sidecar que automaticamente gera spans para requisições HTTP e gRPC interceptadas, propaga contexto de trace através de headers B3 ou W3C conforme configuração, e exporta spans para collectors de tracing como Jaeger ou Zipkin configurados via Telemetry API do Istio, fornecendo visibilidade baseline de topologia de comunicação e latência de rede sem modificações em aplicações legadas não instrumentadas.

Tracing em service mesh captura exclusivamente spans representando comunicação de rede entre serviços através de proxies Envoy: requisição de Service A para Service B resulta em span client-side gerado por Envoy sidecar de A capturando duração de requisição HTTP incluindo tempo de estabelecimento de conexão, serialização de payload e latência de rede, e span server-side gerado por Envoy sidecar de B registrando timestamp de recebimento e encaminhamento para container de aplicação. Limitação fundamental reside em opacidade de processamento interno de aplicação: trace mostra que requisição de Shortener para Analytics consumiu 250ms mas não revela se latência deveu-se a query lenta em ClickHouse, garbage collection pause ou processamento computacionalmente intensivo, exigindo complementação com instrumentação em nível de aplicação para visibilidade end-to-end completa.

Arquitetura híbrida combina tracing de service mesh fornecendo visibilidade de infraestrutura com instrumentação de aplicação capturando lógica de negócio: aplicações instrumentadas com OpenTelemetry criam spans customizados representando operações de domínio enquanto Envoy sidecars automaticamente adicionam spans de rede, context propagation garante que spans de aplicação e infraestrutura correlacionem-se através de Trace ID compartilhado, e visualização resultante apresenta hierarquia completa onde span HTTP de Envoy contém spans filhos de aplicação mostrando decomposição de latência entre overhead de rede capturado por mesh e processamento de negócio instrumentado por SDK. Configuração em Istio habilita tracing através de recurso `Telemetry` especificando provider de tracing e sampling rate: `traceSampling: 1.0` captura 100% de tráfego útil para staging, enquanto `traceSampling: 0.01` aplica 1% sampling em produção reduzindo overhead.

```yaml
# istio-telemetry.yaml
apiVersion: telemetry.istio.io/v1alpha1
kind: Telemetry
metadata:
  name: mesh-tracing
  namespace: istio-system
spec:
  tracing:
    - providers:
        - name: jaeger
      randomSamplingPercentage: 1.0
      customTags:
        cluster_name:
          literal:
            value: production-us-east-1
        environment:
          environment:
            name: ENVIRONMENT
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: istio
  namespace: istio-system
data:
  mesh: |
    defaultConfig:
      tracing:
        zipkin:
          address: jaeger-collector.observability:9411
        tlsSettings:
          mode: DISABLE
    enableTracing: true
    extensionProviders:
      - name: jaeger
        zipkin:
          service: jaeger-collector.observability
          port: 9411
```

## Conclusões

Observabilidade através de tracing distribuído constitui requisito fundamental não opcional para operação efetiva de arquiteturas de microsserviços em produção, onde complexidade inerente de sistemas distribuídos com comunicação síncrona e assíncrona através de múltiplos serviços independentes torna debugging e otimização de performance impossíveis sem visibilidade end-to-end de fluxo de requisições através de topologia de serviços. OpenTelemetry estabelece-se como padrão vendor-neutral unificado fornecendo especificação completa, SDKs maduros e instrumentações automáticas que reduzem drasticamente esforço de adoção permitindo que desenvolvedores habilitem tracing distribuído através de configuração declarativa sem modificações intrusivas em código de aplicação, democratizando acesso a observabilidade avançada anteriormente limitada a organizações com recursos para implementar soluções proprietárias customizadas.

Implementação efetiva de tracing distribuído transcende mera instalação de bibliotecas demandando abordagem holística que integra ferramentas complementares: SDKs de OpenTelemetry instrumentam aplicações gerando spans estruturados, collectors intermediários aplicam transformações e sampling reduzindo volume de dados, backends especializados como Jaeger e Grafana Tempo armazenam e indexam traces habilitando queries eficientes, e plataformas de observabilidade unificada como Grafana correlacionam traces com logs e métricas fornecendo interface única para investigação. Organizações adotando microsserviços devem priorizar investimento em observabilidade desde estágios iniciais de desenvolvimento ao invés de trata-la como afterthought, reconhecendo que custo de implementação de tracing distribuído representa fração mínima comparado a custo de downtime prolongado ou degradação de performance não diagnosticada devido a falta de visibilidade, enquanto benefícios estendem-se além de troubleshooting incluindo otimização de performance baseada em dados reais de produção, validação de SLOs através de análise estatística de latências, e compreensão de dependências de serviços informando decisões arquiteturais sobre decomposição e consolidação de serviços.

## Referências Bibliográficas

OpenTelemetry Official Documentation. _OpenTelemetry: High-quality, ubiquitous, and portable telemetry to enable effective observability_. Cloud Native Computing Foundation, 2025. Disponível em: https://opentelemetry.io/docs/. Acesso em: 26 nov. 2025.

OpenTelemetry JavaScript. _OpenTelemetry JavaScript SDK and Auto-Instrumentation_. OpenTelemetry, 2025. Disponível em: https://github.com/open-telemetry/opentelemetry-js. Acesso em: 26 nov. 2025.

Jaeger Tracing. _Jaeger: open source, distributed tracing platform_. Cloud Native Computing Foundation, 2025. Disponível em: https://www.jaegertracing.io/docs/. Acesso em: 26 nov. 2025.

W3C. _Trace Context - Level 1: W3C Recommendation_. World Wide Web Consortium, 2020. Disponível em: https://www.w3.org/TR/trace-context/. Acesso em: 26 nov. 2025.

SIGELMAN, Benjamin H.; BARROSO, Luiz André; BURROWS, Mike et al. _Dapper, a Large-Scale Distributed Systems Tracing Infrastructure_. Google Technical Report, 2010. Disponível em: https://research.google/pubs/pub36356/. Acesso em: 26 nov. 2025.

Grafana Labs. _Grafana Tempo: high-volume distributed tracing backend_. Grafana Labs, 2025. Disponível em: https://grafana.com/docs/tempo/. Acesso em: 26 nov. 2025.

Istio Documentation. _Distributed Tracing: Understanding your microservices with Istio_. Istio Authors, 2025. Disponível em: https://istio.io/latest/docs/tasks/observability/distributed-tracing/. Acesso em: 26 nov. 2025.

Elastic. _Elastic APM: Application Performance Monitoring_. Elastic N.V., 2025. Disponível em: https://www.elastic.co/guide/en/apm/. Acesso em: 26 nov. 2025.

Datadog. _Distributed Tracing: End-to-end visibility into application performance_. Datadog, Inc., 2025. Disponível em: https://docs.datadoghq.com/tracing/. Acesso em: 26 nov. 2025.

Microsoft Azure. _Distributed Tracing: Microservices patterns and best practices_. Microsoft Corporation, 2025. Disponível em: https://learn.microsoft.com/azure/architecture/microservices/logging-monitoring. Acesso em: 26 nov. 2025.

## Apêndice A: Configuração Completa de Ambiente Local

### Docker Compose para Stack de Observabilidade

```yaml
# docker-compose.observability.yml
version: '3.8'

services:
  jaeger:
    image: jaegertracing/all-in-one:1.52
    container_name: jaeger
    environment:
      - COLLECTOR_OTLP_ENABLED=true
      - COLLECTOR_ZIPKIN_HOST_PORT=:9411
    ports:
      - "16686:16686"  # UI
      - "4317:4317"    # OTLP gRPC
      - "4318:4318"    # OTLP HTTP
      - "14250:14250"  # Jaeger gRPC
      - "9411:9411"    # Zipkin
    networks:
      - observability

  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.91.0
    container_name: otel-collector
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      - ./otel-collector-config.yaml:/etc/otel-collector-config.yaml
    ports:
      - "4317:4317"    # OTLP gRPC
      - "4318:4318"    # OTLP HTTP
      - "8888:8888"    # Metrics
      - "13133:13133"  # Health check
    depends_on:
      - jaeger
    networks:
      - observability

  grafana:
    image: grafana/grafana:10.2.3
    container_name: grafana
    environment:
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana-datasources.yaml:/etc/grafana/provisioning/datasources/datasources.yaml
    ports:
      - "3000:3000"
    networks:
      - observability

  tempo:
    image: grafana/tempo:2.3.1
    container_name: tempo
    command: ["-config.file=/etc/tempo.yaml"]
    volumes:
      - ./tempo-config.yaml:/etc/tempo.yaml
      - tempo-data:/tmp/tempo
    ports:
      - "3200:3200"   # Tempo HTTP
      - "4317:4317"   # OTLP gRPC
    networks:
      - observability

  prometheus:
    image: prom/prometheus:v2.48.1
    container_name: prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    ports:
      - "9090:9090"
    networks:
      - observability

volumes:
  grafana-data:
  tempo-data:
  prometheus-data:

networks:
  observability:
    driver: bridge
```

## Apêndice B: Exemplos de Queries e Análise

### TraceQL para Busca Avançada em Grafana Tempo

```traceql
# Buscar traces com latência acima de 1 segundo
{ duration > 1s }

# Traces com erros em serviço específico
{ resource.service.name = "shortener-service" && status = error }

# Traces contendo operação específica de banco de dados
{ name =~ "pg.query.*" && db.statement =~ ".*urls.*" }

# Traces com múltiplos serviços envolvidos
{ resource.service.name = "api-gateway" }
  && { resource.service.name = "shortener-service" }
  && { resource.service.name = "analytics-service" }

# Análise de latência por endpoint
{ span.http.route = "/api/urls" && span.http.method = "POST" }
  | select(span.duration) | avg()

# Identificar traces com retry patterns
{ span.name =~ ".*retry.*" } | count() > 2
```

## Glossário e Termos Técnicos

**Trace**: Representação completa de jornada de requisição através de sistema distribuído, composta por múltiplos spans correlacionados através de Trace ID único.

**Span**: Unidade atômica de trabalho representando operação individual com timestamps de início e término, atributos estruturados, e relacionamento hierárquico parent-child.

**Trace ID**: Identificador único global de 128 bits propagado através de toda cadeia de chamadas servindo como chave de correlação agrupando spans pertencentes à mesma transação.

**Span ID**: Identificador único de 64 bits de span específico dentro de trace, referenciado por spans filhos através de Parent Span ID estabelecendo hierarquia.

**Context Propagation**: Mecanismo de transmissão de metadados de rastreamento através de boundaries de serviços via headers HTTP, metadados gRPC ou headers de mensagens.

**W3C Trace Context**: Especificação padronizada definindo formato de headers HTTP `traceparent` e `tracestate` para propagação de contexto vendor-neutral.

**OpenTelemetry**: Framework unificado vendor-neutral para observabilidade fornecendo APIs, SDKs e convenções semânticas para traces, métricas e logs.

**Auto-Instrumentation**: Técnica de instrumentação automática que intercepta bibliotecas populares adicionando spans sem modificações em código através de monkey patching ou bytecode manipulation.

**Sampling**: Estratégia de redução de volume de traces através de decisão algorítmica sobre quais traces capturar balanceando visibilidade e custos operacionais.

**Head-Based Sampling**: Decisão de sampling no ponto de entrada quando trace inicia-se, aplicando algoritmo probabilístico antes de conhecimento de outcomes.

**Tail-Based Sampling**: Decisão diferida até conclusão de trace permitindo lógica sofisticada baseando-se em características de execução real como presença de erros.

**Span Events**: Ocorrências timestamped registradas dentro de lifecycle de span capturando eventos pontuais como exceções ou checkpoints sem criar spans filhos.

**Span Attributes**: Pares chave-valor estruturados anexados a spans fornecendo metadados contextuais seguindo convenções semânticas padronizadas.

**OTLP (OpenTelemetry Protocol)**: Protocolo padronizado para transmissão de telemetry data entre aplicações instrumentadas e collectors ou backends.

**Jaeger**: Plataforma open-source de tracing distribuído originalmente desenvolvida por Uber Technologies e graduada pela CNCF.

**Grafana Tempo**: Backend de tracing distribuído otimizado para custos através de armazenamento em object storage econômico como S3.

**Service Mesh**: Camada de infraestrutura dedicada para comunicação inter-serviços através de sidecar proxies fornecendo tracing automático sem instrumentação em aplicação.

**Correlation ID**: Identificador único propagado através de logs e traces permitindo correlação entre sinais de observabilidade distintos para investigação contextual.

**Flame Graph**: Visualização hierárquica de spans apresentando barras horizontais posicionadas temporalmente com largura proporcional à duração e indentação representando parent-child relationships.
